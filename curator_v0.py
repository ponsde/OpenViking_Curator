#!/usr/bin/env python3
import os
import json
import re
import time
from pathlib import Path

import requests
import openviking as ov
from metrics import Metrics
from memory_capture import capture_case

"""
OpenViking Curator v0 (pilot)

Security:
- NO hardcoded API keys
- All secrets loaded from environment variables
"""


def env(name: str, default: str = "") -> str:
    v = os.getenv(name, default)
    return v.strip() if isinstance(v, str) else v


# ---- Config from env ----
OPENVIKING_CONFIG_FILE = env("OPENVIKING_CONFIG_FILE", str(Path.home() / ".openviking" / "ov.conf"))
DATA_PATH = env("CURATOR_DATA_PATH", str(Path.cwd() / "data"))
CURATED_DIR = env("CURATOR_CURATED_DIR", str(Path.cwd() / "curated"))

OAI_BASE = env("CURATOR_OAI_BASE")  # e.g. https://oai.whidsm.cn/v1
OAI_KEY = env("CURATOR_OAI_KEY")

ROUTER_MODELS = [
    m.strip() for m in env(
        "CURATOR_ROUTER_MODELS",
        "gemini-3-flash-preview,gemini-3-flash-high,ã€Claude Codeã€‘Claude-Sonnet 4-5",
    ).split(",") if m.strip()
]
JUDGE_MODEL = env("CURATOR_JUDGE_MODEL", "gemini-3-flash-preview")
JUDGE_MODELS = [
    m.strip() for m in env("CURATOR_JUDGE_MODELS", "gemini-3-flash-preview,gemini-3-flash-high,ã€Claude Codeã€‘Claude-Sonnet 4-5").split(",") if m.strip()
]
ANSWER_MODELS = [
    m.strip() for m in env("CURATOR_ANSWER_MODELS", "gemini-3-flash-preview,gemini-3-flash-high,ã€Claude Codeã€‘Claude-Sonnet 4-5").split(",") if m.strip()
]

GROK_BASE = env("CURATOR_GROK_BASE", "http://127.0.0.1:8000/v1")
GROK_KEY = env("CURATOR_GROK_KEY")
GROK_MODEL = env("CURATOR_GROK_MODEL", "grok-4-fast")


def validate_config() -> None:
    missing = []
    if not OAI_BASE:
        missing.append("CURATOR_OAI_BASE")
    if not OAI_KEY:
        missing.append("CURATOR_OAI_KEY")
    if not GROK_KEY:
        missing.append("CURATOR_GROK_KEY")
    if missing:
        raise RuntimeError(f"Missing required env vars: {', '.join(missing)}")


def chat(base, key, model, messages, timeout=60):
    r = requests.post(
        f"{base}/chat/completions",
        headers={"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
        json={"model": model, "messages": messages, "stream": False},
        timeout=timeout,
    )
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]


def _rule_based_scope(query: str) -> dict:
    """çº¯è§„åˆ™è·¯ç”±ï¼š0 API è°ƒç”¨ï¼Œ<1ms å®Œæˆ"""
    ql = query.lower()

    # â”€â”€ é¢†åŸŸåˆ¤å®š â”€â”€
    _DOMAIN_MAP = {
        "technology": ["docker", "nginx", "linux", "k8s", "kubernetes", "systemd", "git",
                       "python", "asyncio", "rust", "golang", "javascript", "typescript",
                       "api", "mcp", "rag", "llm", "openai", "claude", "grok", "embedding",
                       "vector", "milvus", "chroma", "qdrant", "ci/cd", "github actions",
                       "terraform", "ansible", "openviking", "newapi", "oneapi", "grok2api",
                       "wordpress", "tailscale", "cloudflare", "å‘é‡", "å®¹å™¨", "åå‘ä»£ç†",
                       "éƒ¨ç½²", "é…ç½®", "æ’æŸ¥", "æœåŠ¡å™¨", "æ•°æ®åº“"],
        "devops": ["vps", "ssh", "firewall", "é˜²ç«å¢™", "å®‰å…¨åŠ å›º", "ç›‘æ§", "æ—¥å¿—",
                   "systemctl", "journalctl", "iptables", "ufw"],
    }
    domain = "general"
    for d, terms in _DOMAIN_MAP.items():
        if any(t in ql for t in terms):
            domain = d
            break

    # â”€â”€ å…³é”®è¯æå– â”€â”€
    # è‹±æ–‡æŠ€æœ¯è¯
    en_tokens = re.findall(r"[a-zA-Z0-9_\-/.]{2,}", query)
    # ä¸­æ–‡è¯åˆ‡åˆ†ï¼ˆç®€æ˜“è¯å…¸ + å­—ç¬¦ n-gram å…œåº•ï¼‰
    _CN_TERMS = {
        "æ‰€æœ‰æƒ", "æ¨¡å‹", "ç†è§£", "æ’æŸ¥", "é…ç½®", "æ³¨å†Œ", "å…¥é—¨", "å¯¹æ¯”", "é€‰å‹",
        "å®‰å…¨", "åŠ å›º", "é˜²ç«å¢™", "æ—¥å¿—", "ç½‘ç»œ", "å­˜å‚¨", "å®¹å™¨", "åå‘ä»£ç†",
        "å¸¸è§é—®é¢˜", "æœ€ä½³å®è·µ", "å·¥ä½œåŸç†", "ä½¿ç”¨åœºæ™¯", "è®¾è®¡ç†å¿µ", "å¿«é€Ÿä¸Šæ‰‹",
        "è‡ªåŠ¨æ›´æ–°", "å…¼å®¹æ€§", "å‚æ•°å·®å¼‚", "æ³¨æ„äº‹é¡¹", "ç½‘å…³å¯¹æ¯”", "çŠ¶æ€ç®¡ç†",
        "ä¸Šä¸‹æ–‡", "æ–‡ä»¶ç³»ç»Ÿ", "å‘é‡æ•°æ®åº“", "é™·é˜±",
    }
    cn_tokens = []
    remaining = re.sub(r"[^\u4e00-\u9fff]", "", query)
    while remaining:
        matched = False
        for length in (4, 3, 2):
            if len(remaining) >= length and remaining[:length] in _CN_TERMS:
                cn_tokens.append(remaining[:length])
                remaining = remaining[length:]
                matched = True
                break
        if not matched:
            # è·³è¿‡å•å­—
            remaining = remaining[1:]
    # è¡¥å…… regex 2-gram é˜²æ¼ï¼ˆä½†åªä¿ç•™åœ¨è¯å…¸é‡Œæˆ–æœ‰æ„ä¹‰çš„ï¼‰
    bigrams = re.findall(r"[\u4e00-\u9fff]{2}", query)
    for bg in bigrams:
        if bg in _CN_TERMS and bg not in cn_tokens:
            cn_tokens.append(bg)
    cn_tokens = list(dict.fromkeys(cn_tokens))

    # å»æ‰åœç”¨è¯
    _STOP = {"æ˜¯ä»€ä¹ˆ", "æ€ä¹ˆ", "å¦‚ä½•", "ä»€ä¹ˆ", "å“ªäº›", "å¸¸è§", "æœ‰å“ªäº›", "æœ€ä½³", "å®è·µ",
             "æ€ä¹ˆæ ·", "å¯ä»¥", "åº”è¯¥", "ä¸ºä»€ä¹ˆ", "åˆ°åº•", "ä¸€ä¸‹", "è¿™ä¸ª", "é‚£ä¸ª",
             "the", "what", "how", "is", "are", "and", "for", "with", "to", "in", "of"}
    keywords = [t for t in (en_tokens + cn_tokens) if t.lower() not in _STOP and len(t) > 1]
    # å»é‡ä¿åº
    keywords = list(dict.fromkeys(keywords))[:8]

    # â”€â”€ æ—¶æ•ˆæ€§åˆ¤å®š â”€â”€
    need_fresh = any(k in ql for k in ["æœ€æ–°", "æ›´æ–°", "release", "changelog", "2026", "2025", "latest"])

    return {
        "domain": domain,
        "keywords": keywords,
        "exclude": [],
        "need_fresh": need_fresh,
        "source_pref": ["official_docs", "tech_blog", "github"],
        "confidence": 0.7,
    }


# ç¯å¢ƒå˜é‡æ§åˆ¶ï¼šCURATOR_FAST_ROUTE=1 ç”¨è§„åˆ™ï¼ˆé»˜è®¤ï¼‰ï¼Œ=0 ç”¨ LLM
FAST_ROUTE = env("CURATOR_FAST_ROUTE", "1") == "1"


def route_scope(query: str):
    if FAST_ROUTE:
        return _rule_based_scope(query)

    # LLM fallbackï¼ˆæ…¢ä½†æ›´æ™ºèƒ½ï¼‰
    sys = (
        "ä½ æ˜¯æ£€ç´¢è·¯ç”±å™¨ã€‚æŠŠç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºä¸¥æ ¼JSONï¼Œå­—æ®µ: "
        "domain(å­—ç¬¦ä¸²), keywords(æ•°ç»„), exclude(æ•°ç»„), need_fresh(boolean), source_pref(æ•°ç»„), confidence(0-1)ã€‚"
        "åªè¾“å‡ºJSONï¼Œä¸è¦è§£é‡Šã€‚"
    )
    last_err = None
    out = None
    chosen = None
    for m in ROUTER_MODELS:
        try:
            out = chat(OAI_BASE, OAI_KEY, m, [
                {"role": "system", "content": sys},
                {"role": "user", "content": query},
            ], timeout=45)
            chosen = m
            break
        except Exception as e:
            last_err = e
            continue
    if out is None:
        raise RuntimeError(f"all router models failed: {last_err}")
    print(f"router_model_used={chosen}")

    m = re.search(r"\{[\s\S]*\}", out)
    if not m:
        return {
            "domain": "general",
            "keywords": [query],
            "exclude": [],
            "need_fresh": True,
            "source_pref": ["official docs", "github"],
            "confidence": 0.5,
        }
    try:
        return json.loads(m.group(0))
    except Exception:
        return {
            "domain": "general",
            "keywords": [query],
            "exclude": [],
            "need_fresh": True,
            "source_pref": ["official docs", "github"],
            "confidence": 0.5,
        }


def load_feedback(path: str):
    p = Path(path)
    if p.exists():
        try:
            return json.loads(p.read_text(encoding='utf-8'))
        except Exception:
            return {}
    return {}


def uri_feedback_score(uri: str, fb: dict) -> int:
    if not isinstance(fb, dict):
        return 0

    def _score(item):
        up = int(item.get('up', 0))
        down = int(item.get('down', 0))
        adopt = int(item.get('adopt', 0))
        return up - down + adopt * 2

    # exact match
    if uri in fb:
        return _score(fb[uri])

    # fuzzy match: same subtree / parent-child path overlap
    best = 0
    for k, v in fb.items():
        if not isinstance(k, str):
            continue
        if k in uri or uri in k:
            best = max(best, _score(v))
    return best


def uri_trust_score(uri: str) -> float:
    u = (uri or '').lower()
    s = 5.0
    if 'openviking' in u or 'grok2api' in u or 'newapi' in u:
        s += 1.0
    if 'curated' in u:
        s += 0.5
    if 'license' in u:
        s -= 0.5
    return s


def uri_freshness_score(uri: str) -> float:
    # very light heuristic: curated entries likely newer
    u = (uri or '').lower()
    return 1.0 if 'curated' in u else 0.0


def build_feedback_priority_uris(uris, feedback_file='feedback.json', topn=3):
    fb = load_feedback(feedback_file)
    scored = []
    seen = set()
    for u in uris:
        if u in seen:
            continue
        seen.add(u)
        f = uri_feedback_score(u, fb)             # strong user signal
        t = uri_trust_score(u)                    # weak prior
        r = uri_freshness_score(u)                # freshness prior
        final = 0.50 * f + 0.30 * t + 0.20 * r
        scored.append((final, f, t, r, u))
    scored.sort(reverse=True, key=lambda x: x[0])
    return [x[4] for x in scored[:topn]], scored[:min(5, len(scored))]


def deterministic_relevance(query: str, scope: dict, txt: str, uris: list, domain_hit: bool, kw_cov: float):
    txt_l = (txt or "").lower()
    q_terms = [x for x in re.findall(r"[a-z0-9_\-]{3,}", query.lower()) if x not in {"what", "with", "from", "that"}]
    k_terms = [str(k).lower() for k in scope.get("keywords", [])[:8] if isinstance(k, str)]
    terms = list(dict.fromkeys(q_terms + k_terms))[:12]

    evidence_hit = sum(1 for t in terms if t and t in txt_l)
    evidence_ratio = evidence_hit / max(1, len(terms))

    uri_text = " ".join(uris).lower()
    scope_terms = [str(scope.get("domain", "")).lower()] + [str(x).lower() for x in scope.get("keywords", [])[:4]]
    uri_scope_hit = any(t and t in uri_text for t in scope_terms)

    relevance = 0.55 * kw_cov + 0.30 * evidence_ratio + 0.15 * (1.0 if (domain_hit or uri_scope_hit) else 0.0)
    return relevance, evidence_ratio, uri_scope_hit


def _local_index_search(query: str, kw_list: list, topn: int = 5) -> list:
    """ç”¨æœ¬åœ°å…³é”®è¯ç´¢å¼•å…œåº• OpenViking æ£€ç´¢çš„ä¸ç¨³å®šæ€§"""
    idx_path = os.path.join(os.path.dirname(__file__), '.curated_index.json')
    if not os.path.exists(idx_path):
        return []
    try:
        import json
        idx = json.loads(open(idx_path).read())
    except Exception:
        return []
    ql = query.lower()
    q_terms = set(re.findall(r"[a-z0-9_\-]{2,}", ql)) | set(re.findall(r"[\u4e00-\u9fff]{2,}", query))
    q_terms.update(k.lower() for k in kw_list if k)
    scored = []
    for uri, info in idx.items():
        text = (info.get('title', '') + ' ' + info.get('preview', '')).lower()
        hits = sum(1 for t in q_terms if t in text)
        if hits > 0:
            scored.append((uri, hits, info.get('preview', '')[:1500]))
    scored.sort(key=lambda x: -x[1])
    return scored[:topn]


def local_search(client, query: str, scope: dict):
    # ç¼©å†™å±•å¼€ï¼šçŸ­ç¼©å†™åœ¨è¯­ä¹‰æ£€ç´¢ä¸­å®¹æ˜“è¢«æ·¹æ²¡ï¼Œå±•å¼€å…¨ç§°æå‡å¬å›
    _ABBR_MAP = {
        "mcp": "MCP Model Context Protocol",
        "rag": "RAG Retrieval-Augmented Generation",
        "k8s": "Kubernetes K8s",
        "ci/cd": "CI/CD Continuous Integration Continuous Deployment",
        "llm": "LLM Large Language Model",
        "vlm": "VLM Vision Language Model",
        "oom": "OOM Out Of Memory OOMKilled",
    }
    expanded_q = query
    ql = query.lower()
    for abbr, full in _ABBR_MAP.items():
        if abbr in ql:
            expanded_q = f"{query} {full}"
            break

    expanded = expanded_q + "\nå…³é”®è¯:" + ",".join(scope.get("keywords", [])[:8])

    # åŒè·¯æ£€ç´¢ï¼šfind() è¯­ä¹‰æ›´ç²¾å‡†ï¼Œsearch() è¦†ç›–æ›´å¹¿ï¼Œå–å¹¶é›†
    # å¤šè½®æ£€ç´¢å¯¹å†² OpenViking å‘é‡æ£€ç´¢çš„éšæœºæ€§
    all_items = []
    seen_uris = set()
    search_queries = [expanded]
    if expanded_q != query:
        search_queries.append(expanded_q)  # ç¼©å†™å…¨ç§°ç‰ˆ

    # å¿«é€Ÿæ¨¡å¼ï¼šåªç”¨ search()ï¼ˆçº¯å‘é‡ï¼Œä¸èµ° LLM query planningï¼‰
    # find() æ…¢ä½†æ›´ç²¾å‡†ï¼Œä»…åœ¨éœ€è¦æ—¶ç”¨ä¸€æ¬¡
    methods = [client.search]
    if not FAST_ROUTE:
        methods.insert(0, client.find)

    for sq in search_queries:
        for method in methods:
            try:
                res = method(sq)
                for x in (getattr(res, "resources", []) or []):
                    u = getattr(x, "uri", "")
                    if u and u not in seen_uris:
                        seen_uris.add(u)
                        all_items.append(x)
            except Exception:
                pass

    txt = str(all_items[:5])

    # â”€â”€ æœ¬åœ°ç´¢å¼•å…œåº• â”€â”€
    # OpenViking æ£€ç´¢ä¸ç¨³å®šæ—¶ï¼Œç”¨å…³é”®è¯ç´¢å¼•è¡¥å……å€™é€‰
    idx_hits = _local_index_search(query, scope.get("keywords", []))
    idx_uris_added = set()
    for idx_uri, _, idx_preview in idx_hits:
        if idx_uri not in seen_uris:
            seen_uris.add(idx_uri)
            idx_uris_added.add(idx_uri)
            # åˆ›å»ºä¸€ä¸ªç®€æ˜“ mock å¯¹è±¡
            class _MockResult:
                def __init__(self, u, p):
                    self.uri = u; self.abstract = ''; self._preview = p
            all_items.append(_MockResult(idx_uri, idx_preview))

    # è¿‡æ»¤å™ªå£°
    NOISE_PATTERNS = ("viking://resources/tmp", "/tmp", "tmpr", "å¿«é€Ÿä¸Šæ‰‹",
                      "è®¸å¯è¯", "æ ¸å¿ƒç†å¿µ", "å‰ç½®è¦æ±‚", "/document/content")
    def _is_noise(u: str) -> bool:
        ul = (u or "").lower()
        return any(p in ul for p in NOISE_PATTERNS)

    items = [x for x in all_items
             if str(getattr(x, "uri", "")).startswith("viking://resources")
             and not _is_noise(str(getattr(x, "uri", "")))]

    uris = [getattr(x, "uri", "") for x in items]
    abstracts = [getattr(x, "abstract", "") or "" for x in items]

    # â”€â”€ æ„å»ºå…³é”®è¯åˆ—è¡¨ â”€â”€
    kw = [str(k).strip().lower() for k in scope.get("keywords", [])[:6]
          if isinstance(k, str) and str(k).strip()]
    q_tokens = re.findall(r"[a-z0-9_\-]{2,}", query.lower())
    kw.extend(q_tokens[:6])
    ql = query.lower()

    # â”€â”€ æ ¸å¿ƒè¯ vs é€šç”¨è¯åŒºåˆ† â”€â”€
    # é€šç”¨è¯ï¼šå‡ºç°åœ¨å¤§é‡ä¸åŒä¸»é¢˜æ–‡æ¡£ä¸­ï¼Œä¸èƒ½ä½œä¸ºç›¸å…³æ€§è¯æ®
    _GENERIC_TERMS = {
        "2.0", "3.0", "1.0", "0.1", "2025", "2026", "2024", "æœ€æ–°", "latest",
        "å¯¹æ¯”", "æ¯”è¾ƒ", "åŒºåˆ«", "æœ€ä½³", "å®è·µ", "æ–¹æ¡ˆ", "é€‰å‹", "æ¨è",
        "æ€ä¹ˆ", "å¦‚ä½•", "ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å“ªäº›", "å…¥é—¨", "æŒ‡å—",
        "compare", "best", "practice", "guide", "tutorial", "how",
        "vs", "versus", "performance", "benchmark",
    }
    core_kw = [k for k in kw if k.lower() not in _GENERIC_TERMS and len(k) >= 2]
    generic_kw = [k for k in kw if k.lower() in _GENERIC_TERMS]

    # æ‰‹å·¥é”šç‚¹ï¼ˆé«˜é¢‘å†…éƒ¨æœ¯è¯­ï¼‰
    _anchors = {
        "newapi": ["newapi", "oneapi", "openai", "api gateway"],
        "oneapi": ["newapi", "oneapi", "openai"],
        "mcp": ["mcp", "model context protocol", "tool server"],
        "nginx": ["nginx", "reverse proxy", "upstream", "502", "bad gateway"],
        "docker": ["docker", "container", "dockerfile"],
        "git": ["git", "rebase", "cherry-pick", "reflog"],
        "openviking": ["openviking", "viking", "agfs", "contextual filesystem"],
        "grok2api": ["grok2api", "grok", "auto register", "curated"],
        "asyncio": ["asyncio", "coroutine", "event loop", "await"],
        "github actions": ["github actions", "ci/cd", "workflow", "yaml"],
        "rag": ["rag", "retrieval", "chunk", "rerank", "embedding"],
        "kubernetes": ["kubernetes", "k8s", "pod", "crashloopbackoff"],
        "systemd": ["systemd", "systemctl", "service", "unit file"],
        "claude": ["claude", "anthropic", "openai", "api compatibility"],
        "å‘é‡æ•°æ®åº“": ["vector database", "milvus", "chroma", "qdrant", "weaviate"],
    }
    for anchor_key, anchor_terms in _anchors.items():
        if anchor_key in ql:
            kw.extend(anchor_terms)
    kw = list(dict.fromkeys([k for k in kw if k]))[:16]

    # â”€â”€ æ„å»ºç›¸å…³æ€§æ–‡æœ¬ â”€â”€
    # URI + æ‘˜è¦ + top èµ„æºæ­£æ–‡é¢„è§ˆï¼ˆabstract å¯èƒ½ä¸ºç©ºï¼Œæ‰€ä»¥æ­£æ–‡æ˜¯æ ¸å¿ƒä¿¡å·ï¼‰
    previews = []
    for x in items[:5]:
        u = getattr(x, 'uri', '')
        # ä¼˜å…ˆç”¨ç´¢å¼•ç¼“å­˜çš„ previewï¼Œå…¶æ¬¡ client.read()
        if hasattr(x, '_preview') and x._preview:
            previews.append(x._preview)
        else:
            try:
                content = str(client.read(u))[:1500]
                previews.append(content)
            except Exception:
                pass
    # abstract ä¸ºç©ºæ—¶å®Œå…¨ä¾èµ–æ­£æ–‡
    relevance_text = ("\n".join(uris[:8]) + "\n" + "\n".join(abstracts[:5])
                      + "\n" + "\n".join(previews)).lower()

    hit = sum(1 for k in kw if k in relevance_text)
    kw_cov = hit / max(1, len(kw))

    # â”€â”€ æ ¸å¿ƒè¯è¦†ç›–ç‡ï¼ˆæ›´å‡†ç¡®çš„ç›¸å…³æ€§ä¿¡å·ï¼‰ â”€â”€
    # å¯¹çŸ­è¯ï¼ˆ<=4å­—ç¬¦ï¼‰ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å… "bun" å‘½ä¸­ "ubuntu" ç­‰
    def _core_match(term, text):
        if len(term) <= 4:
            return bool(re.search(r'(?<![a-z])' + re.escape(term) + r'(?![a-z])', text))
        return term in text

    core_hit = sum(1 for k in core_kw if _core_match(k, relevance_text))
    core_cov = core_hit / max(1, len(core_kw)) if core_kw else kw_cov

    # è¯­ä¹‰è¿è´¯æ€§æ£€æŸ¥ï¼šå¦‚æœæ ¸å¿ƒè¯è¦†ç›–ä½ä½†é€šç”¨è¯æ‹‰é«˜äº† kw_covï¼Œæƒ©ç½š
    if core_kw and core_cov < 0.3 and kw_cov > 0.5:
        kw_cov = kw_cov * 0.3  # ä¸¥é‡æƒ©ç½šï¼šæ ¸å¿ƒè¯å‡ ä¹æ²¡å‘½ä¸­

    # â”€â”€ é¢†åŸŸè¯å‘½ä¸­ â”€â”€
    target_terms = []
    for anchor_key, anchor_terms in _anchors.items():
        if anchor_key in ql:
            target_terms.extend(anchor_terms)
    target_terms = list(dict.fromkeys(target_terms))

    full_text = (" ".join(uris) + " " + " ".join(abstracts) + " " + " ".join(previews)).lower()
    domain_hit = any(t in full_text for t in target_terms) if target_terms else False

    relevance, evidence_ratio, uri_scope_hit = deterministic_relevance(
        query, scope, relevance_text, uris, domain_hit, kw_cov)

    # â”€â”€ coverage è®¡ç®— â”€â”€
    effective_domain_hit = (domain_hit
                           or (uri_scope_hit and evidence_ratio >= 0.2)
                           or (relevance >= 0.55 and core_cov >= 0.3))

    # å™ªå£°æƒ©ç½šï¼šè¯æ®å¼±ä½†å…³é”®è¯è¦†ç›–é«˜
    if evidence_ratio < 0.15 and kw_cov > 0.5:
        kw_cov = kw_cov * 0.35

    # æ ¸å¿ƒè¯ç¼ºå¤±æƒ©ç½šï¼šå³ä½¿é€šç”¨è¯å‘½ä¸­å¤šï¼Œæ ¸å¿ƒè¯æ²¡å‘½ä¸­å°±ä¸ç®—çœŸè¦†ç›–
    if core_kw and core_cov < 0.2:
        coverage = min(max(kw_cov, relevance), 0.25) if effective_domain_hit else min(max(kw_cov, relevance), 0.10)
    else:
        coverage = max(kw_cov, relevance) if effective_domain_hit else min(max(kw_cov, relevance), 0.18)

    # curated èµ„æºåŠ æƒï¼šæœåˆ°æˆ‘ä»¬å…¥åº“è¿‡çš„æ–‡æ¡£è¯´æ˜çŸ¥è¯†åº“é‡Œæœ‰ç›¸å…³å†…å®¹
    def _is_our_doc(u):
        ul = u.lower()
        return any(tag in ul for tag in ("curated", "single_", "reingest_", "fix_", "re2_"))
    curated_uris = [u for u in uris if _is_our_doc(u)]
    if curated_uris:
        # ç”¨ query æ ¸å¿ƒè‹±æ–‡è¯ï¼ˆå»æ‰é€šç”¨è¯ï¼‰åœ¨æ­£æ–‡ä¸­åŒ¹é…
        core_en = set(re.findall(r"[a-zA-Z0-9_\-]{3,}", query.lower())) - _GENERIC_TERMS
        core_cn = set(re.findall(r"[\u4e00-\u9fff]{3,4}", query)) - _GENERIC_TERMS
        query_terms = core_en | core_cn
        preview_text = " ".join(previews).lower()
        content_overlap = sum(1 for t in query_terms if t and t.lower() in preview_text)
        overlap_ratio = content_overlap / max(1, len(query_terms))
        if overlap_ratio >= 0.25 or content_overlap >= 3:
            curated_bonus = 0.10 * min(len(curated_uris), 3)
            coverage = max(coverage, 0.40) + curated_bonus
            coverage = min(1.0, coverage)

    # æœ¬åœ°ç´¢å¼•å¼ºå…œåº•ï¼šå¦‚æœç´¢å¼•å‘½ä¸­äº†é«˜ç›¸å…³æ–‡æ¡£ä½† OpenViking æ£€ç´¢éšæœºæ€§å¯¼è‡´ coverage ä½
    if coverage < 0.45:
        idx_results = _local_index_search(query, kw)
        if idx_results:
            best_hits = idx_results[0][1]
            best_preview = idx_results[0][2].lower()
            # è‡³å°‘ 3 ä¸ªå…³é”®è¯å‘½ä¸­æ‰ç®—å¼ºåŒ¹é…
            if best_hits >= 3:
                idx_terms = set(kw) | set(re.findall(r"[\u4e00-\u9fff]{2,}", query))
                idx_overlap = sum(1 for t in idx_terms if t and t.lower() in best_preview)
                if idx_overlap >= 2:
                    coverage = max(coverage, 0.50)

    # feedback è°ƒæƒ
    fb = load_feedback(os.getenv('CURATOR_FEEDBACK_FILE', 'feedback.json'))
    uri_scores = {u: uri_feedback_score(u, fb) for u in uris[:20]}
    max_fb = max(uri_scores.values()) if uri_scores else 0
    if max_fb > 0:
        coverage = min(1.0, coverage + 0.08 * max_fb)

    pri_uris, rank_preview = build_feedback_priority_uris(
        uris, os.getenv('CURATOR_FEEDBACK_FILE', 'feedback.json'), topn=3)

    top_trust = [x[2] for x in rank_preview[:3]] if rank_preview else []
    avg_top_trust = (sum(top_trust) / len(top_trust)) if top_trust else 0.0
    fresh_ratio = (len(curated_uris) / max(1, min(8, len(uris)))) if uris else 0.0

    return txt, coverage, {
        "kw_cov": round(kw_cov, 3),
        "core_cov": round(core_cov, 3),
        "domain_hit": effective_domain_hit,
        "target_terms": target_terms[:6],
        "uris": uris[:8],
        "max_feedback_score": max_fb,
        "priority_uris": pri_uris,
        "rank_preview": rank_preview,
        "relevance": round(relevance, 3),
        "evidence_ratio": round(evidence_ratio, 3),
        "uri_scope_hit": uri_scope_hit,
        "avg_top_trust": round(avg_top_trust, 3),
        "fresh_ratio": round(fresh_ratio, 3),
    }


def external_boost_needed(query: str, scope: dict, coverage: float, meta: dict):
    q = (query or "").lower()
    need_fresh = bool(scope.get("need_fresh", False)) or any(k in q for k in ["æœ€æ–°", "æ›´æ–°", "release", "changelog", "2026", "2025"])
    low_quality = meta.get("avg_top_trust", 0) < 5.4
    low_fresh = meta.get("fresh_ratio", 0) < 0.25
    weak_feedback = meta.get("max_feedback_score", 0) <= 0
    core_cov = meta.get("core_cov", 1.0)

    # è¦†ç›–ç‡é˜ˆå€¼ï¼ˆå·²çŸ¥å†…éƒ¨åŸŸåå¯æ›´å®½æ¾ï¼Œå‡å°‘é‡å¤å¤–æœï¼‰
    low_cov_threshold = 0.45
    if any(k in q for k in ["newapi", "openviking", "grok2api", "mcp"]):
        low_cov_threshold = 0.35

    if coverage < low_cov_threshold:
        return True, "low_coverage"
    # æ ¸å¿ƒè¯è¦†ç›–ä½ = çŸ¥è¯†åº“å¯¹è¿™ä¸ªè¯é¢˜å®é™…æ²¡è¦†ç›–ï¼Œå³ä½¿é€šç”¨è¯æ‹‰é«˜äº† coverage
    if core_cov <= 0.4:
        return True, "low_core_coverage"
    if need_fresh and (low_fresh or low_quality):
        return True, "freshness_or_quality_boost"
    if need_fresh and weak_feedback and low_quality:
        return True, "need_fresh_no_positive_feedback"
    return False, "local_sufficient"


def external_search(query: str, scope: dict):
    import datetime
    today = datetime.date.today().isoformat()
    prompt = (
        f"é—®é¢˜: {query}\n"
        f"å…³é”®è¯: {scope.get('keywords', [])}\n"
        f"æ’é™¤: {scope.get('exclude', [])}\n"
        f"åå¥½æ¥æº: {scope.get('source_pref', [])}\n"
        f"å½“å‰æ—¥æœŸ: {today}\n\n"
        "è¦æ±‚:\n"
        "1. è¿”å›5æ¡é«˜è´¨é‡æ¥æºï¼Œæ ¼å¼ï¼šæ ‡é¢˜+URL+å‘å¸ƒ/æ›´æ–°æ—¥æœŸ+å…³é”®ç‚¹\n"
        "2. ä¼˜å…ˆæœ€è¿‘6ä¸ªæœˆå†…çš„ä¿¡æ¯ï¼Œæ ‡æ³¨æ¯æ¡æ¥æºçš„æ—¥æœŸ\n"
        "3. å¦‚æœå¼•ç”¨çš„é¡¹ç›®/æ–‡æ¡£è¶…è¿‡1å¹´æœªæ›´æ–°ï¼Œæ˜ç¡®æ ‡æ³¨[å¯èƒ½è¿‡æ—¶]\n"
        "4. æ¶‰åŠAPIã€æ³¨å†Œæµç¨‹ã€è®¤è¯æ–¹å¼ç­‰æ˜“å˜å†…å®¹æ—¶ï¼Œå¿…é¡»ç¡®è®¤å½“å‰æ˜¯å¦ä»ç„¶æœ‰æ•ˆ\n"
        "5. ä¸è¦æŠŠæ—§ç‰ˆæœ¬çš„æŠ€æœ¯è¦æ±‚å½“æˆå½“å‰äº‹å®ï¼ˆå¦‚å·²å–æ¶ˆçš„éªŒè¯æ­¥éª¤ï¼‰\n"
        "6. GitHubé¡¹ç›®å¿…é¡»æ ‡æ³¨ï¼šæœ€åcommitæ—¥æœŸã€staræ•°ã€æ˜¯å¦archived\n"
        "7. åŒºåˆ†[å¯ç›´æ¥ä½¿ç”¨]å’Œ[ä»…ä¾›å‚è€ƒ]â€”â€”ç»´æŠ¤ä¸­ä¸”æœ‰æ–‡æ¡£çš„æ‰ç®—å¯ç”¨"
    )
    return chat(GROK_BASE, GROK_KEY, GROK_MODEL, [
        {"role": "system", "content": (
            "ä½ æ˜¯å®æ—¶æœç´¢åŠ©æ‰‹ã€‚é‡è§†å¯éªŒè¯æ¥æºå’Œä¿¡æ¯æ—¶æ•ˆæ€§ã€‚"
            f"å½“å‰æ—¥æœŸ: {today}ã€‚"
            "å¯¹äºæŠ€æœ¯ç±»é—®é¢˜ï¼Œä¼˜å…ˆå¼•ç”¨å®˜æ–¹æ–‡æ¡£å’Œè¿‘æœŸæ›´æ–°ã€‚"
            "å¦‚æœæœåˆ°çš„ä¿¡æ¯å¯èƒ½å·²è¿‡æ—¶ï¼ˆå¦‚è¶…è¿‡1å¹´çš„é¡¹ç›®ã€å·²å˜æ›´çš„APIæµç¨‹ï¼‰ï¼Œ"
            "å¿…é¡»æ˜ç¡®æ ‡æ³¨å¹¶æç¤ºç”¨æˆ·éªŒè¯ã€‚"
            "å¯¹äºGitHubé¡¹ç›®ï¼ŒåŠ¡å¿…åŒºåˆ†ï¼šé¡¹ç›®å­˜åœ¨ â‰  é¡¹ç›®èƒ½ç”¨ã€‚"
        )},
        {"role": "user", "content": prompt},
    ], timeout=90)


def cross_validate(query: str, external_text: str, scope: dict) -> dict:
    """P0: äº¤å‰éªŒè¯ + é“¾å¼æœç´¢
    æ£€æµ‹å¤–æœç»“æœä¸­çš„æ˜“å˜å£°æ˜ï¼Œè‡ªåŠ¨è¿½é—®éªŒè¯ã€‚
    è¿”å›: {"validated": str, "warnings": list, "followup_done": bool}
    """
    import datetime
    today = datetime.date.today().isoformat()

    # ç¬¬ä¸€æ­¥ï¼šç”¨ LLM è¯†åˆ«å¤–æœç»“æœä¸­éœ€è¦éªŒè¯çš„å£°æ˜
    extract_prompt = (
        f"å½“å‰æ—¥æœŸ: {today}\n\n"
        f"ä»¥ä¸‹æ˜¯å…³äºã€Œ{query}ã€çš„å¤–éƒ¨æœç´¢ç»“æœ:\n{external_text[:3000]}\n\n"
        "è¯·è¯†åˆ«å…¶ä¸­çš„ã€Œæ˜“å˜å£°æ˜ã€â€”â€”å³å¯èƒ½å·²ç»è¿‡æ—¶æˆ–éœ€è¦éªŒè¯çš„æŠ€æœ¯äº‹å®ã€‚\n"
        "é‡ç‚¹å…³æ³¨:\n"
        "- APIç«¯ç‚¹ã€æ³¨å†Œ/è®¤è¯æµç¨‹ã€éªŒè¯è¦æ±‚ï¼ˆè¿™äº›ç»å¸¸å˜ï¼‰\n"
        "- æ¥è‡ªè¶…è¿‡6ä¸ªæœˆå‰çš„é¡¹ç›®çš„æŠ€æœ¯å£°æ˜\n"
        "- å¤šä¸ªæ¥æºä¹‹é—´äº’ç›¸çŸ›ç›¾çš„è¯´æ³•\n"
        "- æŠŠæŸä¸ªé¡¹ç›®çš„ç‰¹å®šå®ç°å½“æˆé€šç”¨äº‹å®çš„æƒ…å†µ\n\n"
        "è¾“å‡ºä¸¥æ ¼JSON: {\"claims\": [{\"claim\": \"...\", \"source_date\": \"...\", \"risk\": \"high/medium/low\"}], "
        "\"needs_followup\": bool, \"followup_query\": \"å¦‚æœneeds_followup=trueï¼Œç»™å‡ºéªŒè¯æœç´¢è¯\"}"
    )

    try:
        # å°è¯•å¤šä¸ªæ¨¡å‹ï¼Œé˜²æ­¢å•ç‚¹ 503
        cv_models = (JUDGE_MODELS if JUDGE_MODELS else []) + ["gemini-3-flash-preview"]
        out = None
        for cv_model in cv_models:
            try:
                out = chat(OAI_BASE, OAI_KEY, cv_model, [
                    {"role": "system", "content": "ä½ æ˜¯ä¿¡æ¯éªŒè¯å™¨ã€‚è¯†åˆ«éœ€è¦äº¤å‰éªŒè¯çš„æ˜“å˜æŠ€æœ¯å£°æ˜ã€‚åªè¾“å‡ºJSONã€‚"},
                    {"role": "user", "content": extract_prompt},
                ], timeout=45)
                break
            except Exception as e:
                print(f"  âš ï¸ cross_validate model {cv_model} failed: {e}")
                continue

        if not out:
            return {"validated": external_text, "warnings": [], "followup_done": False}

        match = re.search(r"\{[\s\S]*\}", out)
        if not match:
            return {"validated": external_text, "warnings": [], "followup_done": False}

        result = json.loads(match.group(0))
        claims = result.get("claims", [])
        high_risk = [c for c in claims if c.get("risk") == "high"]
        warnings = [c.get("claim", "") for c in high_risk]

        # ç¬¬äºŒæ­¥ï¼šå¦‚æœæœ‰é«˜é£é™©å£°æ˜ä¸”å»ºè®®è¿½é—®ï¼Œåšé“¾å¼æœç´¢
        followup_text = ""
        if result.get("needs_followup") and result.get("followup_query") and high_risk:
            print(f"  ğŸ”„ äº¤å‰éªŒè¯: è¿½é—® â†’ {result['followup_query']}")
            try:
                followup_text = chat(GROK_BASE, GROK_KEY, GROK_MODEL, [
                    {"role": "system", "content": (
                        f"ä½ æ˜¯å®æ—¶æœç´¢åŠ©æ‰‹ã€‚å½“å‰æ—¥æœŸ: {today}ã€‚"
                        "è¯·æœç´¢æœ€æ–°å®˜æ–¹ä¿¡æ¯æ¥éªŒè¯ä»¥ä¸‹å£°æ˜æ˜¯å¦ä»ç„¶æˆç«‹ã€‚"
                        "ä¼˜å…ˆå¼•ç”¨å®˜æ–¹æ–‡æ¡£ã€Help Centerã€Release Notesã€‚"
                    )},
                    {"role": "user", "content": (
                        f"éœ€è¦éªŒè¯çš„å£°æ˜:\n" +
                        "\n".join([f"- {c.get('claim','')}" for c in high_risk]) +
                        f"\n\néªŒè¯æœç´¢: {result['followup_query']}"
                    )},
                ], timeout=60)
                print(f"  âœ… è¿½é—®å®Œæˆ: {len(followup_text)} chars")
            except Exception as e:
                print(f"  âš ï¸ è¿½é—®å¤±è´¥: {e}")

        # åˆå¹¶ç»“æœ
        validated = external_text
        if followup_text:
            validated = (
                external_text +
                "\n\n--- äº¤å‰éªŒè¯è¡¥å…… ---\n" +
                followup_text
            )

        return {
            "validated": validated,
            "warnings": warnings,
            "followup_done": bool(followup_text),
            "high_risk_count": len(high_risk),
        }

    except Exception as e:
        print(f"  âš ï¸ äº¤å‰éªŒè¯å¼‚å¸¸: {e}")
        return {"validated": external_text, "warnings": [], "followup_done": False}


def judge_and_pack(query: str, external_text: str):
    import datetime
    today = datetime.date.today().isoformat()
    sys = (
        "ä½ æ˜¯èµ„æ–™å®¡æ ¸å™¨ã€‚åˆ¤æ–­å¤–éƒ¨æœç´¢ç»“æœæ˜¯å¦å€¼å¾—å…¥åº“ã€‚\n"
        f"å½“å‰æ—¥æœŸ: {today}\n\n"
        "å®¡æ ¸ç»´åº¦:\n"
        "1. å†…å®¹å‡†ç¡®æ€§ â€” ä¿¡æ¯æ˜¯å¦æ­£ç¡®ã€æ˜¯å¦æœ‰æ¥æºæ”¯æ’‘\n"
        "2. æ—¶æ•ˆæ€§ â€” ä¿¡æ¯æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼ŸAPIæµç¨‹/æ³¨å†Œæ–¹å¼/æŠ€æœ¯è¦æ±‚ç­‰æ˜“å˜å†…å®¹å°¤å…¶æ³¨æ„\n"
        "   - è¶…è¿‡1å¹´æœªæ›´æ–°çš„é¡¹ç›®ä¿¡æ¯ï¼štrusté™ä½ï¼Œæ ‡æ³¨[å¯èƒ½è¿‡æ—¶]\n"
        "   - å¼•ç”¨å·²å–æ¶ˆ/å˜æ›´çš„åŠŸèƒ½å½“ä½œå½“å‰äº‹å®ï¼špass=false\n"
        "   - å°†æ—§ç‰ˆæœ¬è¦æ±‚ï¼ˆå¦‚å·²å–æ¶ˆçš„æ‰‹æœºéªŒè¯ï¼‰å½“æˆç°è¡Œè¦æ±‚ï¼špass=false\n"
        "3. å…¥åº“ä»·å€¼ â€” æ˜¯å¦å€¼å¾—é•¿æœŸä¿å­˜ï¼Œè¿˜æ˜¯åªæ˜¯ä¸´æ—¶å‚è€ƒ\n\n"
        "è¾“å‡ºä¸¥æ ¼JSON: pass(bool), reason(string), tags(array), trust(0-10), "
        "freshness(string: current/recent/outdated/unknown), "
        "summary(string), markdown(string)ã€‚\n"
        "markdownè¦æ±‚åŒ…å«æ¥æºURLå’Œä¿¡æ¯æ—¥æœŸã€‚åªè¾“å‡ºJSONã€‚"
    )

    last_err = None
    out = None
    for jm in JUDGE_MODELS:
        try:
            print(f"judge_model_used={jm}")
            out = chat(OAI_BASE, OAI_KEY, jm, [
                {"role": "system", "content": sys},
                {"role": "user", "content": f"ç”¨æˆ·é—®é¢˜:{query}\nå€™é€‰èµ„æ–™:\n{external_text}"},
            ], timeout=90)
            break
        except Exception as e:
            last_err = e
            continue

    if out is None:
        return {"pass": False, "reason": f"judge_model_fail:{last_err}", "tags": [], "trust": 0, "summary": "", "markdown": ""}

    m = re.search(r"\{[\s\S]*\}", out)
    if not m:
        return {"pass": False, "reason": "bad_json", "tags": [], "trust": 0, "summary": "", "markdown": ""}
    try:
        return json.loads(m.group(0))
    except Exception:
        return {"pass": False, "reason": "json_parse_fail", "tags": [], "trust": 0, "summary": "", "markdown": ""}


def ingest_markdown(client, title: str, markdown: str, freshness: str = "unknown"):
    import datetime
    p = Path(CURATED_DIR)
    p.mkdir(parents=True, exist_ok=True)

    # P2: å…¥åº“æ—¶å†™å…¥ metadataï¼ˆæ—¥æœŸ + æ—¶æ•ˆæ ‡ç­¾ï¼‰
    today = datetime.date.today().isoformat()
    ttl_map = {"current": 180, "recent": 90, "unknown": 60, "outdated": 0}
    ttl_days = ttl_map.get(freshness, 60)

    header = (
        f"<!-- curator_meta: ingested={today} freshness={freshness} ttl_days={ttl_days} -->\n"
        f"<!-- review_after: {(datetime.date.today() + datetime.timedelta(days=ttl_days)).isoformat()} -->\n\n"
    )

    fn = p / f"{int(time.time())}_{re.sub(r'[^a-zA-Z0-9_-]+', '_', title)[:40]}.md"
    fn.write_text(header + markdown, encoding="utf-8")
    ing = client.add_resource(path=str(fn))

    # å…³é”®ä¿®å¤ï¼šå…¥åº“åç­‰å¾…è¯­ä¹‰ç´¢å¼•å®Œæˆï¼Œå¦åˆ™ä¸‹ä¸€æ¬¡æ£€ç´¢æ‹¿ä¸åˆ°æ–°æ–‡æ¡£
    try:
        uri = ing.get("root_uri", "") if isinstance(ing, dict) else ""
        if uri:
            client.wait_processed()  # ä¸ä¼ å‚ï¼šç­‰å…¨éƒ¨é˜Ÿåˆ—å®Œæˆ
    except Exception:
        pass

    return ing


def build_priority_context(client, uris):
    blocks = []
    for u in uris[:2]:
        try:
            c = client.read(u)
            blocks.append(f"[PRIORITY_SOURCE] {u}\n{str(c)[:1200]}")
        except Exception:
            continue
    return "\n\n".join(blocks)


def detect_conflict(query: str, local_ctx: str, external_ctx: str):
    if not external_ctx.strip():
        return {"has_conflict": False, "summary": "", "points": []}

    sys = (
        "ä½ æ˜¯å†²çªæ£€æµ‹å™¨ã€‚æ¯”è¾ƒæœ¬åœ°ä¸Šä¸‹æ–‡ä¸å¤–éƒ¨è¡¥å……æ˜¯å¦å­˜åœ¨ç»“è®ºå†²çªã€‚"
        "è¾“å‡ºä¸¥æ ¼JSONï¼šhas_conflict(bool), summary(string), points(array of string)ã€‚"
        "å¦‚æœåªæ˜¯ç»†èŠ‚å·®å¼‚ä½†ä¸å½±å“ç»“è®ºï¼Œhas_conflict=falseã€‚åªè¾“å‡ºJSONã€‚"
    )
    out = chat(OAI_BASE, OAI_KEY, JUDGE_MODEL, [
        {"role": "system", "content": sys},
        {"role": "user", "content": f"é—®é¢˜:{query}\n\næœ¬åœ°:\n{local_ctx[:2500]}\n\nå¤–éƒ¨:\n{external_ctx[:2500]}"},
    ], timeout=60)
    m = re.search(r"\{[\s\S]*\}", out)
    if not m:
        return {"has_conflict": False, "summary": "", "points": []}
    try:
        j = json.loads(m.group(0))
        if "points" not in j or not isinstance(j.get("points"), list):
            j["points"] = []
        return j
    except Exception:
        return {"has_conflict": False, "summary": "", "points": []}


def answer(query: str, local_ctx: str, external_ctx: str, priority_ctx: str = "",
           conflict_card: str = "", warnings: list = None):
    import datetime
    today = datetime.date.today().isoformat()
    warning_block = ""
    if warnings:
        warning_block = "\nâš ï¸ ä»¥ä¸‹ä¿¡æ¯éœ€è°¨æ…å¯¹å¾…ï¼ˆå¯èƒ½è¿‡æ—¶æˆ–æœªç»éªŒè¯ï¼‰:\n" + "\n".join([f"- {w}" for w in warnings[:5]])

    sys = (
        f"ä½ æ˜¯æŠ€æœ¯åŠ©æ‰‹ã€‚å½“å‰æ—¥æœŸ: {today}ã€‚åŸºäºç»™å®šä¸Šä¸‹æ–‡å›ç­”ã€‚\n"
        "è§„åˆ™:\n"
        "1. æœ€åç»™æ¥æºåˆ—è¡¨ï¼Œæ ‡æ³¨æ¯ä¸ªæ¥æºçš„æ—¥æœŸ\n"
        "2. è‹¥å­˜åœ¨å†²çªå¡ç‰‡ï¼Œå…ˆå±•ç¤ºå†²çªå†ç»™å»ºè®®\n"
        "3. å¯¹äºä¸ç¡®å®šçš„ä¿¡æ¯ï¼Œæ˜ç¡®æ ‡æ³¨ã€Œâš ï¸ å¾…éªŒè¯ã€\n"
        "4. å¼•ç”¨è¶…è¿‡1å¹´çš„èµ„æ–™æ—¶ï¼Œæé†’å¯èƒ½è¿‡æ—¶\n"
        "5. åŒºåˆ†ã€Œç»è¿‡éªŒè¯çš„äº‹å®ã€å’Œã€Œæ¥è‡ªç¬¬ä¸‰æ–¹é¡¹ç›®çš„å®ç°ç»†èŠ‚ã€\n"
        "6. å¦‚æœæœ‰è­¦å‘Šä¿¡æ¯ï¼Œåœ¨å›ç­”å¼€å¤´æç¤ºç”¨æˆ·æ³¨æ„"
    )
    user = (
        f"é—®é¢˜:\n{query}\n\n"
        f"{warning_block}\n\n"
        f"å†²çªå¡ç‰‡:\n{conflict_card}\n\n"
        f"ä¼˜å…ˆæ¥æºä¸Šä¸‹æ–‡:\n{priority_ctx[:2500]}\n\n"
        f"æœ¬åœ°ä¸Šä¸‹æ–‡:\n{local_ctx[:5000]}\n\n"
        f"å¤–éƒ¨è¡¥å……:\n{external_ctx[:3000]}"
    )

    last_err = None
    for m in ANSWER_MODELS:
        try:
            print(f"answer_model_used={m}")
            return chat(OAI_BASE, OAI_KEY, m, [
                {"role": "system", "content": sys},
                {"role": "user", "content": user},
            ], timeout=90)
        except Exception as e:
            last_err = e
            continue
    raise RuntimeError(f"all answer models failed: {last_err}")


def _build_source_footer(meta: dict, coverage: float, external_used: bool,
                         warnings: list = None) -> str:
    """ç”Ÿæˆå›ç­”åº•éƒ¨çš„æ¥æºé€æ˜åº¦ä¿¡æ¯"""
    lines = ["---", "ğŸ“Š **å›ç­”è´¨é‡ä¿¡æ¯**"]

    # è¦†ç›–ç‡
    cov_pct = int(coverage * 100)
    if cov_pct >= 80:
        cov_label = "âœ… é«˜"
    elif cov_pct >= 50:
        cov_label = "âš ï¸ ä¸­ç­‰"
    else:
        cov_label = "âŒ ä½"
    lines.append(f"- çŸ¥è¯†åº“è¦†ç›–ç‡: {cov_pct}% ({cov_label})")
    lines.append(f"- æ ¸å¿ƒè¯è¦†ç›–: {meta.get('core_cov', '?')}")

    # æ¥æº
    if external_used:
        lines.append("- æ¥æº: æœ¬åœ°çŸ¥è¯†åº“ + å¤–éƒ¨æœç´¢ï¼ˆå·²äº¤å‰éªŒè¯ï¼‰")
    else:
        lines.append("- æ¥æº: æœ¬åœ°çŸ¥è¯†åº“")

    # ä½¿ç”¨çš„èµ„æº
    uris = meta.get('priority_uris', [])
    if uris:
        short_uris = [u.split('/')[-1].replace('.md', '') for u in uris[:3]]
        lines.append(f"- ä¸»è¦å‚è€ƒ: {', '.join(short_uris)}")

    # è­¦å‘Š
    if warnings:
        lines.append(f"- âš ï¸ æœ‰ {len(warnings)} æ¡å¾…éªŒè¯ä¿¡æ¯")

    return "\n".join(lines)


def run(query: str):
    m = Metrics()
    validate_config()
    os.environ["OPENVIKING_CONFIG_FILE"] = OPENVIKING_CONFIG_FILE

    print("STEP 1/8 åˆå§‹åŒ–...")
    client = ov.SyncOpenViking(path=DATA_PATH)
    client.initialize()
    m.step('init', True)
    print("âœ… STEP 1 å®Œæˆ")

    try:
        print("STEP 2/8 èŒƒå›´è·¯ç”±...")
        scope = route_scope(query)
        m.step('route', True, {'domain': scope.get('domain'), 'confidence': scope.get('confidence')})
        m.score('router_confidence', scope.get('confidence', 0))
        print("âœ… STEP 2 å®Œæˆ:", json.dumps(scope, ensure_ascii=False))

        print("STEP 3/8 æœ¬åœ°æ£€ç´¢(OpenViking)...")
        local_txt, coverage, meta = local_search(client, query, scope)
        m.step('local_search', True, {'coverage': coverage, 'kw_cov': meta.get('kw_cov'), 'domain_hit': meta.get('domain_hit')})
        m.score('coverage_before_external', round(coverage, 3))
        print(
            f"âœ… STEP 3 å®Œæˆ: coverage={coverage:.2f}, kw_cov={meta['kw_cov']:.2f}, core_cov={meta.get('core_cov', '?')}, "
            f"domain_hit={meta['domain_hit']}, relevance={meta.get('relevance')}, evidence={meta.get('evidence_ratio')}, "
            f"avg_trust={meta.get('avg_top_trust')}, fresh_ratio={meta.get('fresh_ratio')}, fb_max={meta.get('max_feedback_score',0)}, "
            f"priority_uris={meta.get('priority_uris',[])}, rank_preview={meta.get('rank_preview',[])}, "
            f"target_terms={meta['target_terms']}, uris={meta.get('uris', [])}"
        )

        external_txt = ""
        ingested = False
        boost_needed, boost_reason = external_boost_needed(query, scope, coverage, meta)
        if boost_needed:
            m.flag('external_triggered', True)
            m.flag('external_reason', boost_reason)
            print(f"STEP 4/8 è§¦å‘å¤–éƒ¨æœç´¢(Grok)... reason={boost_reason}")
            external_txt = external_search(query, scope)
            m.step('external_search', True, {'len': len(external_txt), 'reason': boost_reason})
            print("âœ… STEP 4 å®Œæˆ: å¤–éƒ¨ç»“æœé•¿åº¦", len(external_txt))

            print("STEP 5/8 äº¤å‰éªŒè¯...")
            cv = cross_validate(query, external_txt, scope)
            external_txt = cv.get("validated", external_txt)
            cv_warnings = cv.get("warnings", [])
            m.step('cross_validate', True, {
                'followup_done': cv.get('followup_done', False),
                'high_risk_count': cv.get('high_risk_count', 0),
                'warnings': cv_warnings[:3],
            })
            if cv_warnings:
                print(f"  âš ï¸ äº¤å‰éªŒè¯è­¦å‘Š: {cv_warnings}")
            else:
                print("  âœ… æ— é«˜é£é™©å£°æ˜")

            print("STEP 6/8 å®¡æ ¸å¹¶å°è¯•å…¥åº“...")
            j = judge_and_pack(query, external_txt)
            m.step('judge', True, {'pass': j.get('pass'), 'trust': j.get('trust')})
            print("å®¡æ ¸ç»“æœ:", json.dumps({k: j.get(k) for k in ["pass", "reason", "trust", "tags", "freshness"]}, ensure_ascii=False))
            if j.get("pass") and j.get("markdown"):
                # æ—¶æ•ˆæ€§æ‹¦æˆªï¼šoutdated çš„ä¿¡æ¯ä¸å…¥åº“
                freshness = j.get("freshness", "unknown")
                if freshness == "outdated":
                    m.step('ingest', False, {'reason': 'outdated_info'})
                    print("âš ï¸ æœªå…¥åº“: ä¿¡æ¯å·²è¿‡æ—¶ (freshness=outdated)")
                else:
                    ing = ingest_markdown(client, "curated", j["markdown"], freshness=freshness)
                    ingested = True
                    m.step('ingest', True, {'uri': ing.get('root_uri', '')})
                    print("âœ… å·²å…¥åº“:", ing.get("root_uri", ""))
            else:
                m.step('ingest', False)
                print("âš ï¸ æœªå…¥åº“")
        else:
            m.flag('external_triggered', False)
            m.flag('external_reason', boost_reason)
            cv_warnings = []
            print("STEP 4/8 è·³è¿‡å¤–éƒ¨æœç´¢ï¼ˆæœ¬åœ°è¦†ç›–ä¸è´¨é‡è¶³å¤Ÿï¼‰")

        print("STEP 7/8 å†²çªæ£€æµ‹...")
        conflict = detect_conflict(query, local_txt, external_txt)
        conflict_card = ""
        if conflict.get('has_conflict'):
            pts = '\n'.join([f"- {x}" for x in conflict.get('points', [])[:5]])
            conflict_card = f"âš ï¸ å­˜åœ¨å†²çª: {conflict.get('summary','')}\n{pts}"
        m.step('conflict', True, {'has_conflict': conflict.get('has_conflict', False), 'summary': conflict.get('summary','')})
        m.flag('has_conflict', bool(conflict.get('has_conflict', False)))
        print(f"âœ… STEP 7 å®Œæˆ: has_conflict={bool(conflict.get('has_conflict', False))}")

        print("STEP 8/8 ç”Ÿæˆå›ç­”...")
        priority_ctx = build_priority_context(client, meta.get('priority_uris', []))
        ans = answer(query, local_txt, external_txt, priority_ctx=priority_ctx,
                     conflict_card=conflict_card, warnings=cv_warnings)

        # å›ç­”é€æ˜åº¦ï¼šé™„åŠ æ¥æºå’Œç½®ä¿¡åº¦ä¿¡æ¯
        source_info = _build_source_footer(meta, coverage, boost_needed, cv_warnings)
        ans = ans.rstrip() + "\n\n" + source_info
        m.step('answer', True, {'answer_len': len(ans), 'priority_uris': meta.get('priority_uris', [])})
        m.score('priority_uris_count', len(meta.get('priority_uris', [])))
        m.flag('ingested', ingested)
        m.score('answer_len', len(ans))
        report = m.finalize()

        case_path = None
        if os.getenv('CURATOR_CAPTURE_CASE', '1') in ('1','true','True'):
            case_path = capture_case(query, scope, report, ans, out_dir=os.getenv('CURATOR_CASE_DIR','cases'))

        print("\n===== FINAL ANSWER =====\n")
        print(ans)
        print("\n===== EVAL METRICS =====\n")
        print(json.dumps({
            'duration_sec': report['duration_sec'],
            'flags': report['flags'],
            'scores': report['scores'],
            'case_path': case_path
        }, ensure_ascii=False, indent=2))
    finally:
        try:
            client.close()
        except Exception:
            pass


if __name__ == "__main__":
    import sys

    q = " ".join(sys.argv[1:]).strip() or "grok2api è‡ªåŠ¨æ³¨å†Œéœ€è¦å“ªäº›å‰ç½®é…ç½®å’Œå¸¸è§å¤±è´¥åŸå› ï¼Ÿ"
    run(q)
